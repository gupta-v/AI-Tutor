Aconvolutional neural network(CNN) is a type offeedforward neural networkthat learnsfeaturesviafilter(or kernel) optimization. This type ofdeep learningnetwork has been applied to process and makepredictionsfrom many different types of data including text, images and audio.[1]Convolution-based networks are the de-facto standard indeep learning-based approaches tocomputer vision[2]and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as thetransformer.
Vanishing gradientsand exploding gradients, seen duringbackpropagationin earlier neural networks, are prevented by theregularizationthat comes from using shared weights over fewer connections.[3][4]For example, foreachneuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascadedconvolution(or cross-correlation) kernels,[5][6]only 25 weights for each convolutional layer are required to process 5x5-sized tiles.[7][8]Higher-layer features are extracted from wider context windows, compared to lower-layer features.
Some applications of CNNs include:
CNNs are also known asshift invariantorspace invariant artificial neural networks, based on the shared-weight architecture of theconvolutionkernels or filters that slide along input features and provide translation-equivariantresponses known as feature maps.[14][15]Counter-intuitively, most convolutional neural networks are notinvariant to translation, due to the downsampling operation they apply to the input.[16]
Feedforward neural networksare usually fully connected networks, that is, each neuron in onelayeris connected to all neurons in the nextlayer. The "full connectivity" of these networks makes them prone tooverfittingdata. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[17]
Convolutional networks wereinspiredbybiologicalprocesses[18][19][20][21]in that the connectivity pattern betweenneuronsresembles the organization of the animalvisual cortex. Individualcortical neuronsrespond to stimuli only in a restricted region of thevisual fieldknown as thereceptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to otherimage classification algorithms. This means that the network learns to optimize thefilters(or kernels) through automated learning, whereas in traditional algorithms these filters arehand-engineered. This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks.
A convolutional neural network consists of an input layer,hidden layersand an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs adot productof the convolution kernel with the layer's input matrix. This product is usually theFrobenius inner product, and its activation function is commonlyReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such aspooling layers, fully connected layers, and normalization layers.
Here it should be noted how close a convolutional neural network is to amatched filter.[22]
In a CNN, the input is atensorwith shape:
(number of inputs) × (input height) × (input width) × (inputchannels)