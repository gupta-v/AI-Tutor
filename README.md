# KIRA: AI Tutor - Smart Multimodal Learning Assistant

**Version-1**

## Description

**KIRA** is an intelligent, multimodal educational assistant built using **Flask** and advanced **AI models**, designed to provide precise and engaging learning support. Whether you're uploading PDFs for reference or seeking real-time answers through web scraping, AI Tutor adapts dynamically using **Retrieval-Augmented Generation (RAG)** and **Agentic capabilities** to ensure tailored responses.

## Tech Stack

| Layer                     | Technologies Used                                            |
| ------------------------- | ------------------------------------------------------------ |
| **Frontend**              | Flask, HTML, CSS, JavaScript                                 |
| **AI Models**             | Gemini 1.5-flash, Gemini 2.0-flash, mxbai-embed-large:latest |
| **Vector Datastore**      | FAISS (in-memory vector store)                               |
| **Web Scraping**          | Langchain Tools, BeautifulSoup4, SERP                        |
| **RAG + Agentic Support** | Langchain, GoogleGenerativeAI, webbrowser                    |
| **NLP Processing**        | Ollama Embeddings, CharacterTextSplitter, NLTK               |
| **Voice Interaction**     | SpeechRecognition, pyttsx3 (TTS)                             |

## Features

### ğŸ” Dual Response Capability

- **With PDFs**: Extracts content from uploaded PDFs, embeds them into FAISS using cosine similarity, and generates responses by augmenting model outputs.
- **Without PDFs**: Uses real-time web scraping and curates links from the web to answer queries dynamically.

### ğŸ§  Intelligent Context Handling

- Differentiates between **retrieval mode** (document-based) and **non-retrieval mode** (web-based), ensuring the most context-aware and relevant responses.

### ğŸ”— RAG Capabilities

- Retrieves embedded vector chunks using cosine similarity and enhances Gemini-generated outputs with this specific context.

### ğŸ¤– Agentic Capabilities

- Automates browsing using web scraping tools.
- Opens recommended learning links in the user's browser for detailed study.

### ğŸ™ï¸ Voice-Enabled Learning

- Listens to queries using **speech recognition**.
- Responds using **text-to-speech**, improving accessibility and engagement.

### ğŸ§© Multi-Model Chaining

- Chains multiple models during RAG processing:
  | Model | Purpose |
  |-------------------|---------------------------------------------------------------------------------------------------------------|
  | **shunya_llm** | Generation for non-retrieved queries using user input + web-scraped content. |
  | **pratham_llm** | Retrieval-based topic generation using vector DB or document content. |
  | **dviteey_llm** | Cross-verification of pratham_llm's output using retrieved data to reduce hallucinations and enhance clarity. |

### ğŸ“š Resource-Driven Recommendations

- Offers curated links or document-based responses with no redundancyâ€”focused only on what's essential for the user.

## ğŸ“ File Structure (with Descriptions)

```plaintext
AI-TUTOR/
â”œâ”€â”€ aiFeatures/
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ ai_assistant.py              # Core logic to manage query routing and decision-making
â”‚       â”œâ”€â”€ ai_response.py               # Handles AI model responses and augmentation
â”‚       â”œâ”€â”€ rag_pipeline.py              # Implements RAG (retrieval-augmented generation) process
â”‚       â”œâ”€â”€ speech_to_text.py            # Converts voice input to text using speech recognition
â”‚       â”œâ”€â”€ text_to_speech.py            # Converts text responses to speech using pyttsx3
â”‚       â”œâ”€â”€ web_scraper_tool.py          # Automates scraping tools using BeautifulSoup and SERP
â”‚       â””â”€â”€ web_scraping.py              # Generalized web scraping logic for live content retrieval
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ scrapings/
â”‚       â””â”€â”€ scraped_content.txt          # Stores temporarily scraped content from the web
â”‚
â”œâ”€â”€ testFrontend/
â”‚   â””â”€â”€ FlaskApp/
â”‚       â”œâ”€â”€ static/
â”‚       â”‚   â”œâ”€â”€ script.js                # JavaScript for dynamic frontend interaction
â”‚       â”‚   â””â”€â”€ style.css                # Custom styling for the frontend
â”‚       â””â”€â”€ templates/
â”‚           â””â”€â”€ index.html               # Main HTML template rendered by Flask
â”‚
â”œâ”€â”€ app.py                               # Entry-point Flask server script integrating backend/frontend
â”œâ”€â”€ .env                                 # Environment file for storing API keys and secrets
â”œâ”€â”€ .gitignore                           # Git ignore rules
â”œâ”€â”€ README.md                            # Project documentation
â””â”€â”€ requirements.txt                     # Python dependencies
```

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/gupta-v/AI-Tutor.git
cd AI-Tutor
```

### 2. Set Up Virtual Environment

```bash
python -m venv env
source env/bin/activate  # or `env\Scripts\activate` on Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment Variables

Create a `.env` file and add your API keys:

```env
GEMINI_API_KEY=your_gemini_key
SERPAPI_KEY=your_serpapi_key
```

### 5. Run the Application

```bash
python app.py
```

Access the app at: [http://localhost:5000](http://localhost:5000)

## Ollama Setup

To use **Ollama embeddings** for document chunking and vector representation, follow these steps:

### 1. Install Ollama

Download and install Ollama from the official site:

ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

After installation, ensure it is accessible in your terminal:

```bash
ollama --version
```

### 2. Pull the Required Model

Pull a supported embedding model like `mxbai-embed-large` or any other compatible model:

```bash
ollama pull mxbai-embed-large
```

### 3. Start Ollama Server (if required)

Ollama typically runs as a background service. If not, you can start it manually:

```bash
ollama serve
```

### 4. Integration in AI Tutor

The application uses Ollama to embed chunks of PDF/text using:

- `mxbai-embed-large` or any embedding model you configure
- Embeddings are stored in FAISS vector store and used for cosine similarity-based retrieval

Ensure your `.env` or config file includes proper references to use Ollama embeddings.

```env
OLLAMA_MODEL=mxbai-embed-large
```

Youâ€™re now ready to use Ollama with AI TutorğŸŒŸ

## Example Usage

### Voice Query

> "Explain Newton's second law"

- âœ… Converts to text â†’ Checks for PDF â†’ Fetches embedded context (if available) â†’ Responds via Gemini + TTS

### Text Query Without Upload

> "What is quantum entanglement?"

- âŒ No documents â†’ ğŸŒ Web scraping triggered â†’ ğŸ§  Top 3 results shown + ğŸ”Š Voice explanation

### Text Query With Upload

> Upload a PDF â†’ Ask "Summarize Chapter 2"

- âœ… FAISS retrieves PDF chunks â†’ Gemini refines answer using those â†’ ğŸ§ Output is spoken

## To-Do / Future Enhancements

- âœ… Add image-based query support (Multimodal)
- ğŸ”„ Integrate quiz and flashcards generation from uploaded materials
- ğŸ”’ Secure backend in GOlang/Python
- ğŸš€ Scaling and deployment using Docker + Firebase

## Credits

- Developed by Vaibhav Gupta, Shweta Maurya, Shreya Pandey, Vartika Upadhyay
- Built using Google's Gemini API, FAISS, Langchain, and Ollama
- Open-source contributions are welcome ğŸ¤
